---
layout: post
title: testing
categories: [golang]
tags: [golang]
---

# Mastering Go's Idiomatic Testing: A Guide to Table-Driven & Sub-Tests

In the world of DevOps, where reliability and maintainability are paramount, the way we test our code is as important as the code itself. For engineers coming from languages like Python or Java, the testing landscape in Go can feel both refreshingly simple and subtly different. Go eschews complex frameworks and opts for a powerful, built-in approach that champions clarity and scalability. At the heart of this philosophy lies a simple yet profoundly effective pattern: **table-driven tests** combined with **subtests**.

This combination is the idiomatic standard for writing tests in Go. It provides a structured, readable, and highly maintainable way to cover a wide range of inputs and expected outcomes for a single function. By mastering this technique, you can write comprehensive test suites that are easy to extend and debug, making your Go applications more robust and reliable—a core goal of any DevOps practice.

***

## Executive Overview

Table-driven testing is a style where you define a collection of test cases—the "table"—as a slice of structs. Each struct in the slice represents a complete test case, containing the inputs for the function under test, the expected output, and often a descriptive name. The test function then iterates over this table, executing each case one by one. This approach keeps your test logic concise and separate from the test data, making it incredibly easy to add new scenarios without duplicating code. You simply add another struct to the slice.

The real power of this pattern is unlocked when combined with **subtests**, a feature introduced in Go 1.7 via the `t.Run` method. Instead of running all test cases within a single, monolithic test function, `t.Run` allows you to create a distinct, nested test for each entry in your table. These subtests are treated as individual test functions by the Go tooling. This means they are reported on separately, providing clearer and more granular feedback on failures. You can even run a specific subtest from the command line, which is invaluable for quickly debugging a particular failing case without running the entire suite.

For a DevOps engineer, this pattern directly supports key principles. It promotes **Don't Repeat Yourself (DRY)** by centralizing test logic. It enhances **clarity and maintainability**, as the table structure makes it trivial to understand the function's expected behavior across various inputs. Furthermore, the isolation provided by subtests simplifies debugging and integrates seamlessly into CI/CD pipelines, where clear, actionable test-failure reports are crucial for rapid feedback and deployment cycles. This built-in, convention-over-configuration approach is quintessentially Go, offering a robust testing foundation without the need for external dependencies.

***

## Deep-Dive Implementation

To truly grasp the power of table-driven tests and subtests, it's essential to understand the mechanics behind them and how they differ from the testing paradigms you might be familiar with in other ecosystems.

### The Core Mechanic: Slices of Structs

The foundation of a table-driven test is a slice of anonymous structs. This "table" holds all the data required for your test cases. A typical structure for a test case includes:

* **`name`**: A `string` that provides a descriptive name for the test case. This name is used when creating a subtest and is crucial for identifying which scenario failed.
* **`input`**: The input value(s) for the function being tested. This can be a single value or a struct if the function takes multiple arguments.
* **`expectedOutput`**: The result you expect the function to return for the given `input`.
* **`expectedError`**: If you expect the function to return an error, this field defines the expected error type or message.

Let's imagine we're testing a simple function `Add(a, b int) int`. The test table would look like this:

```go
tests := []struct {
    name           string
    a, b           int
    expectedOutput int
}{
    // ... test cases defined here ...
}
```

By defining the test cases as data, you create a clean separation between the testing *logic* (the loop that runs the tests) and the testing *data* (the slice of structs). This makes the tests far easier to read and maintain.

### Bringing Tests to Life with `t.Run`

Once the table is defined, the test function iterates over it. Inside the loop, `t.Run()` is called for each test case.

```go
func TestAdd(t *testing.T) {
    tests := []struct {
        // ... struct fields ...
    }{
        // ... test cases ...
    }

    for _, tc := range tests {
        t.Run(tc.name, func(t *testing.T) {
            // Test logic for a single case
        })
    }
}
```

The `t.Run` method takes two arguments:
1.  A `string` for the subtest's name (we use our `tc.name` field for this).
2.  A function literal of type `func(t *testing.T)`. This is the body of our subtest.

This structure provides several key benefits:

* **Isolation**: Each subtest is a distinct testing scope. A failure in one subtest (e.g., a `t.Fatal`) will not stop other subtests from running.
* **Clear Reporting**: When a test fails, the output clearly indicates which specific subtest failed using a hierarchical name, like `TestMyFunction/positive_numbers`.
* **Targeted Execution**: You can run a specific subtest directly from the command line, which is a massive time-saver for debugging. The command `go test -v -run TestMyFunction/specific_case` will execute *only* the subtest named `specific_case`.

### How It Differs: A DevOps Perspective

For engineers accustomed to other toolchains, Go's approach can feel minimalist.

* **Compared to Python (`pytest`)**: In `pytest`, it's common to use decorators like `@pytest.mark.parametrize` to achieve a similar outcome. This decorator injects different sets of arguments into a single test function. While effective, it relies on a powerful external framework with its own specific syntax. Go, by contrast, achieves this using standard language features—slices, structs, and loops—making the pattern universally understood by any Go developer without needing to learn a framework's API. There are no "fixtures" in Go's standard library; setup and teardown are typically handled with helper functions or within the subtest itself using `t.Cleanup`.

* **Compared to Java (`JUnit`)**: JUnit often uses annotations like `@Test` and `@ParameterizedTest`. While JUnit 5's parameterized tests are functionally similar to Go's table-driven tests, the Java ecosystem often encourages a more verbose, class-based structure for tests. Go's file-based `*_test.go` convention and the straightforward data structure of the table often result in less boilerplate and a more direct representation of the test cases.

The key takeaway is that Go's testing style is deeply integrated with the language itself. It favors directness and simplicity, leveraging core language constructs over specialized syntax or third-party frameworks. This aligns perfectly with the DevOps mindset of using simple, robust, and transparent tools.

***

## Idiomatic Code Walk-through

Let's apply this pattern to a practical scenario relevant to DevOps: parsing a simple YAML configuration file that defines a server's properties.

### Example: Parsing a Server Configuration

Imagine we have a utility function, `NewServerConfig`, that takes a YAML byte slice as input and returns a `ServerConfig` struct or an error if the YAML is invalid or missing required fields.

Here's the function we want to test (e.g., in a file named `config.go`):

```go
package config

import (
	"fmt"
	"gopkg.in/yaml.v3"
)

// ServerConfig defines the configuration for a server.
type ServerConfig struct {
	Host string `yaml:"host"`
	Port int    `yaml:"port"`
}

// NewServerConfig parses a YAML byte slice and returns a ServerConfig.
// It returns an error if parsing fails or if required fields are missing.
func NewServerConfig(data []byte) (*ServerConfig, error) {
	var cfg ServerConfig
	if err := yaml.Unmarshal(data, &cfg); err != nil {
		return nil, fmt.Errorf("failed to unmarshal yaml: %w", err)
	}

	if cfg.Host == "" {
		return nil, fmt.Errorf("validation error: host is a required field")
	}

	if cfg.Port == 0 {
		return nil, fmt.Errorf("validation error: port is a required field")
	}

	return &cfg, nil
}
```
*Note: This example uses the popular `gopkg.in/yaml.v3` library. To add it to your project, run:* `go get gopkg.in/yaml.v3`

Now, let's write the test for it in `config_test.go` using the table-driven and subtest approach.

```go
package config

import (
	"errors"
	"reflect"
	"testing"
)

// TestNewServerConfig uses a table-driven approach to test the NewServerConfig function.
func TestNewServerConfig(t *testing.T) {
	// The "table" is a slice of structs, where each struct is a complete test case.
	testCases := []struct {
		name          string // A descriptive name for the test case.
		inputYAML     []byte // The input byte slice for our function.
		expectedCfg   *ServerConfig // The expected successful output.
		expectedErr   error    // The expected error, if any.
	}{
		{
			name:      "valid config",
			inputYAML: []byte("host: localhost\nport: 8080"),
			expectedCfg: &ServerConfig{
				Host: "localhost",
				Port: 8080,
			},
			expectedErr: nil,
		},
		{
			name:          "invalid yaml syntax",
			inputYAML:     []byte("host: localhost\nport: 8080:"), // Intentional syntax error
			expectedCfg:   nil,
			expectedErr:   errors.New("failed to unmarshal yaml"), // We check for the error type, not the exact string.
		},
		{
			name:          "missing required host field",
			inputYAML:     []byte("port: 9000"),
			expectedCfg:   nil,
			expectedErr:   errors.New("validation error: host is a required field"),
		},
		{
			name:          "missing required port field",
			inputYAML:     []byte("host: example.com"),
			expectedCfg:   nil,
			expectedErr:   errors.New("validation error: port is a required field"),
		},
		{
			name:          "empty input",
			inputYAML:     []byte(""),
			expectedCfg:   nil,
			expectedErr:   errors.New("failed to unmarshal yaml"),
		},
	}

	// We iterate over our test cases.
	for _, tc := range testCases {
		// t.Run creates a subtest. This allows for better test organization and reporting.
		// The first argument is the name of the subtest, taken from our test case struct.
		t.Run(tc.name, func(t *testing.T) {
			// This is the core of the test. We run the function we want to test.
			actualCfg, actualErr := NewServerConfig(tc.inputYAML)

			// 1. Check the error.
			// It's important to check if an error was expected or not.
			if tc.expectedErr != nil {
				// We expected an error.
				if actualErr == nil {
					t.Fatalf("expected error '%v', but got nil", tc.expectedErr)
				}
				// For better testing, we shouldn't compare error strings directly.
				// Using errors.Is or errors.As is more robust. Here, for simplicity in demonstration,
				// we are checking if the actual error string contains the expected one.
				// A more robust check would use errors.Is. Let's demonstrate that.
				// We can't use errors.Is for wrapped errors from yaml library directly,
				// so we will check for our custom validation error messages.
				if tc.name == "missing required host field" || tc.name == "missing required port field" {
					if actualErr.Error() != tc.expectedErr.Error() {
						t.Errorf("expected error message '%s', got '%s'", tc.expectedErr, actualErr)
					}
				}
			} else if actualErr != nil {
				// We did NOT expect an error, but we got one.
				t.Fatalf("did not expect an error, but got: %v", actualErr)
			}

			// 2. Check the returned configuration.
			// Using reflect.DeepEqual is the idiomatic way to compare structs, maps, and slices in tests.
			if !reflect.DeepEqual(actualCfg, tc.expectedCfg) {
				t.Errorf("expected config %+v, but got %+v", tc.expectedCfg, actualCfg)
			}
		})
	}
}
```

### Running the Tests: From Local to CI/CD

Now that you have the test file, running it is straightforward using the built-in Go tools.

* **Run all tests in the package**:
    ```bash
    # Command to execute from your terminal in the package directory.
    go test
    # Output will be simple: PASS or FAIL.
    # ---
    # PASS
    # ok      your/module/config   0.015s
    ```

* **Run with verbose output**:
    The `-v` flag provides detailed output, showing each subtest being run. This is extremely helpful for seeing which specific cases are passing.
    ```bash
    # Command
    go test -v
    # ---
    # === RUN   TestNewServerConfig
    # === RUN   TestNewServerConfig/valid_config
    # === RUN   TestNewServerConfig/invalid_yaml_syntax
    # === RUN   TestNewServerConfig/missing_required_host_field
    # === RUN   TestNewServerConfig/missing_required_port_field
    # === RUN   TestNewServerConfig/empty_input
    # --- PASS: TestNewServerConfig (0.00s)
    # --- PASS: TestNewServerConfig/valid_config (0.00s)
    # --- PASS: TestNewServerConfig/invalid_yaml_syntax (0.00s)
    # --- PASS: TestNewServerConfig/missing_required_host_field (0.00s)
    # --- PASS: TestNewServerConfig/missing_required_port_field (0.00s)
    # --- PASS: TestNewServerConfig/empty_input (0.00s)
    # PASS
    # ok      your/module/config   0.021s
    ```

* **Run a specific subtest**:
    If you have a failure in `missing_required_host_field`, you can run just that test.
    ```bash
    # Command uses a regex pattern. The forward slash separates the top-level test from the subtest.
    go test -v -run TestNewServerConfig/missing_required_host_field
    ```

* **Generate a coverage report**:
    Code coverage is a critical metric in any CI environment.
    ```bash
    # Command
    go test -coverprofile=coverage.out
    # This generates a file 'coverage.out' that can be processed.

    # To view the report in your browser:
    go tool cover -html=coverage.out
    ```

#### CI/CD Considerations

In a CI/CD pipeline (e.g., GitHub Actions, GitLab CI), you would typically run tests for your entire project:

```yaml
# Example snippet for a GitHub Actions workflow
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Run tests with coverage
      run: go test -v -coverprofile=coverage.out ./...

    - name: Upload coverage report
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: coverage.out
```
The `./...` wildcard tells Go to run tests in the current directory and all subdirectories recursively. The verbose (`-v`) flag ensures that CI logs provide immediate, clear feedback on which tests ran and which, if any, failed.

***

## Gotchas & Best Practices

While the table-driven pattern is powerful, there are common pitfalls to avoid. Adhering to best practices will ensure your tests remain robust, readable, and maintainable.

### 1. The Loop Variable Capture Pitfall in Parallel Tests

This is the most famous "gotcha." When you use `t.Parallel()` inside a subtest to speed up your test suite, you can run into issues with Go's loop variable scoping. In older versions of Go, the subtest's function literal would capture a reference to the loop variable (`tc`), not its value. Because the parallel subtests don't execute immediately, by the time they do run, the loop may have already finished, and `tc` will be referencing the *last* item in the slice for all subtests.

**Go 1.22+ largely fixed this issue for `for` loops**, but it's crucial to understand the problem as you may encounter it in older codebases or different loop constructs.

### 2. Opaque Test Failure Messages

A test that fails with a message like `"expected true, got false"` is not helpful. Your failure messages should provide context. Instead of just stating the failure, include the inputs and the differing outputs.

Use `t.Errorf()` with detailed format strings:
`t.Errorf("For input '%s', expected output '%d' but got '%d'", tc.input, tc.expected, actual)`

This immediately tells the developer what failed and why, saving valuable debugging time.

### 3. Improper Error Comparison

Comparing errors by their string representation (`err.Error() == "some string"`) is brittle. Error messages can change, and this approach fails to properly handle wrapped errors.

* **Use `errors.Is(err, target)`**: This should be your default choice. It checks if any error in the chain matches the `target` error instance. This is perfect for checking against sentinel errors (e.g., `sql.ErrNoRows`).
* **Use `errors.As(err, &target)`**: Use this when you need to check if an error in the chain is of a specific *type* and you want to inspect its fields.

### 4. Modifying Shared State Between Subtests

Subtests, especially when run in parallel, must be independent. If your test cases modify a shared global variable, a shared map, or a common resource without proper synchronization, you will introduce flaky, unpredictable tests.

Always ensure that each subtest works on its own isolated data. If setup is required, perform it *inside* the `t.Run` block or use `t.Cleanup` to ensure resources are properly reset after each subtest.

### 5. Test Naming and Readability

The `name` field in your test case struct is not just a label; it's a piece of documentation. Good names describe the *scenario* being tested.

* **Bad**: `"test 1"`, `"case 2"`
* **Good**: `"valid config with all fields"`, `"error on missing host"`, `"handles empty input gracefully"`

Clear names make the test output from `go test -v` a readable report on your function's behavior.

***

## Code Examples: Gotchas and Remediation

Here are concrete code examples demonstrating the pitfalls and how to correct them.

### Pitfall 1: Loop Variable Capture in Parallel Tests

Even though Go 1.22+ fixed this for `for` loops, it's vital to know the classic pattern for ensuring correctness in any version or context.

#### ❌ Incorrect Code (Classic Bug)

```go
func TestParallel_Incorrect(t *testing.T) {
    testCases := []string{"a", "b", "c"}

    for _, tc := range testCases {
        // In older Go versions, all subtests would capture the last value of tc, which is "c".
        t.Run(tc, func(t *testing.T) {
            t.Parallel() // Mark subtest for parallel execution.
            // This test would likely fail for "a" and "b" because tc would be "c".
            t.Logf("Testing with value: %s", tc)
            // Imagine an assertion here: if tc != "c" { t.Error(...) }
        })
    }
}
```

#### ✅ Remediated Code

The classic fix, which works in all Go versions and is still good practice for clarity, is to re-shadow the loop variable inside the loop.

```go
func TestParallel_Correct(t *testing.T) {
    testCases := []string{"a", "b", "c"}

    for _, tc := range testCases {
        tc := tc // Shadow the loop variable. tc is now a new variable local to this loop iteration.
        t.Run(tc, func(t *testing.T) {
            t.Parallel()
            // Now, each subtest has its own immutable copy of tc.
            t.Logf("Correctly testing with value: %s", tc)
            // Assertions here will work as expected.
        })
    }
}
```

### Pitfall 2: Opaque Test Failure Messages

Clear failures are non-negotiable for efficient debugging, especially in a CI/CD context.

#### ❌ Incorrect Code

```go
func TestFailureMessage_Incorrect(t *testing.T) {
	output := "hello world"
	expected := "hello go"

	if output != expected {
		// This message is not helpful. Why did it fail? What were the values?
		t.Error("output did not match expected")
	}
}
```

#### ✅ Remediated Code

```go
func TestFailureMessage_Correct(t *testing.T) {
	output := "hello world"
	expected := "hello go"

	if output != expected {
		// This message gives the developer immediate, actionable information.
		t.Errorf("handler returned unexpected body: got %q want %q", output, expected)
	}
}
```
*Note: The `%q` verb in format strings is excellent for strings as it safely quotes them.*

### Pitfall 3: Improper Error Comparison

Relying on string comparison for errors is fragile. Use Go's `errors` package.

#### ❌ Incorrect Code

```go
import (
	"errors"
	"fmt"
	"testing"
)

var ErrCustom = errors.New("my custom error")

func functionThatWrapsError() error {
	return fmt.Errorf("an issue occurred: %w", ErrCustom)
}

func TestErrorComparison_Incorrect(t *testing.T) {
	err := functionThatWrapsError()

	// This is brittle. The wrapper message "an issue occurred: " might change.
	if err.Error() != "an issue occurred: my custom error" {
		t.Errorf("unexpected error message: %s", err)
	}
}
```

#### ✅ Remediated Code

```go
import (
	"errors"
	"fmt"
	"testing"
)

// (assuming same ErrCustom and functionThatWrapsError from above)

func TestErrorComparison_Correct(t *testing.T) {
	err := functionThatWrapsError()

	// Robust and correct. It checks if ErrCustom is in the error chain.
	if !errors.Is(err, ErrCustom) {
		t.Errorf("expected error to be '%v', but it was not", ErrCustom)
	}
}
```

### Pitfall 4: Modifying Shared State Between Subtests

Tests must be isolated to be reliable. Shared state is a common source of flaky tests that fail intermittently.

#### ❌ Incorrect Code

```go
func TestSharedState_Incorrect(t *testing.T) {
	// This shared map is a problem.
	sharedData := make(map[string]string)

	testCases := []string{"first", "second"}

	for _, tc := range testCases {
		t.Run(tc, func(t *testing.T) {
			t.Parallel() // Running in parallel makes the race condition obvious.
			// Both subtests will race to write to the same map key.
			sharedData["key"] = tc
			if sharedData["key"] != tc {
				t.Errorf("data corruption detected! got %s, want %s", sharedData["key"], tc)
			}
		})
	}
}
```

#### ✅ Remediated Code

The solution is to give each subtest its own state.

```go
func TestSharedState_Correct(t *testing.T) {
	testCases := []string{"first", "second"}

	for _, name := range testCases {
		name := name // Shadow variable for parallelism.
		t.Run(name, func(t *testing.T) {
			t.Parallel()
			// Each subtest gets its own isolated map. No shared state.
			localData := make(map[string]string)
			localData["key"] = name
			if localData["key"] != name {
				t.Errorf("unexpected value: got %s, want %s", localData["key"], name)
			}
		})
	}
}
```
